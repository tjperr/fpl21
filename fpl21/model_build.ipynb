{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "\n",
    "import json\n",
    "import random\n",
    "import matplotlib as pyplot\n",
    "from tqdm import tqdm\n",
    "\n",
    "import fpl21.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate base data set\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.merge(player_attrs, on='element')\n",
    "\n",
    "# df = df.merge(team_data, left_on='opponent_team', right_index=True) \\\n",
    "#    .rename(columns={c: f\"opp_{c}\" for c in team_cols}) \\\n",
    "#    .merge(team_data, left_on='team', right_index=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Static player attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_attrs_df = data.create_player_attrs_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Historical match data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = data.create_history_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Team data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_df = data.create_team_stats_df()\n",
    "team_cols = list(team_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = history_df.merge(player_attrs_df, on='element') \\\n",
    "    .merge(team_df, left_on='opponent_team', right_index=True) \\\n",
    "    .rename(columns={c: f\"opp_{c}\" for c in team_cols}) \\\n",
    "    .merge(team_data, left_on='team', right_index=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['element', 'web_name', 'fixture', 'round',]\n",
    "\n",
    "# Static player attributes\n",
    "player_cols = ['element_type']\n",
    "changing_player_cols = ['chance_of_playing_this_round', 'ep_this', 'ep_next']\n",
    "\n",
    "# Attributes from fixture data\n",
    "fixtures_cols = ['is_home', 'team', 'opponent_team'] + team_cols + [f\"opp_{c}\" for c in team_cols]\n",
    "\n",
    "# Historical match data - contains outcomes and things that are correlated, e.g. number of minutes played\n",
    "# Need to be lagged to use as predictors\n",
    "history_cols = ['total_points', 'bonus', 'bps', 'minutes', 'selected', 'transfers_in', 'transfers_out'] \n",
    "\n",
    "df = df[labels + player_cols + changing_player_cols + fixtures_cols + history_cols]\n",
    "\n",
    "# Others\n",
    "\n",
    "# 'goals_scored', 'assists', 'clean_sheets', 'goals_conceded',\n",
    "# 'own_goals', 'penalties_saved', 'penalties_missed', 'yellow_cards',\n",
    "# 'red_cards', 'saves', 'influence', 'creativity',\n",
    "# 'threat', 'ict_index', 'value', 'transfers_balance',\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.total_points.hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature generation\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_avg(df, window, col, default):\n",
    "    rolling = pd.Series(dtype='float64')\n",
    "    for x in df.element.unique():\n",
    "        rolling = rolling.append(df[df.element==x].sort_values('fixture').rolling(window)[col].mean().shift(1))\n",
    "    \n",
    "    # Fill nas with default val\n",
    "    rolling = rolling.fillna(default)\n",
    "    \n",
    "    rolling.name = f\"avg_{col}_L{window}\"\n",
    "    return df.merge(rolling, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in tqdm(history_cols): # we can use previous outcomes to predict next values\n",
    "    df = rolling_avg(df, 1, var, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_index().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use data where rolling metrics are populated\n",
    "df = df[df['round'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.element_type = df.element_type.astype(str)\n",
    "df.team = df.team.astype(str)\n",
    "df.opponent_team = df.opponent_team.astype(str)\n",
    "df.chance_of_playing_this_round = df.chance_of_playing_this_round.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covars = [name for name in df.columns if name not in labels + history_cols + ['ep_this', 'ep_next']]\n",
    "\n",
    "X = df[covars]\n",
    "y = df.total_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom train/test split to ensure all player observations in same set and prevent overstating test performance\n",
    "# Loop as performance on test has high variance\n",
    "perf_results = []\n",
    "for _ in tqdm(range(10)):\n",
    "    elements = list(df.element.unique())\n",
    "    test_elements = random.sample(elements, len(elements) // 4)\n",
    "    mask = np.array([(x in test_elements) for x in df.element])\n",
    "\n",
    "    X_train = X[~mask]\n",
    "    y_train = y[~mask]\n",
    "    X_test = X[mask]\n",
    "    y_test = y[mask]\n",
    "\n",
    "    train = (X_train, y_train)\n",
    "    test = (X_test, y_test)\n",
    "#     print(X_train.shape, X_test.shape)\n",
    "\n",
    "    reg = RandomForestRegressor(min_samples_split=10, criterion='mse')\n",
    "\n",
    "    reg.fit(*train)\n",
    "    perf_results.append((reg.score(*train), reg.score(*test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(perf_results, columns=['Train', 'Test']).describe().loc[['min', 'mean', 'max']].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(reg.feature_importances_, index=covars, columns=['importance']) \\\n",
    "    .sort_values('importance').plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Log (Test performance)\n",
    "\n",
    "# 0.4000 - using ep_this, ep_next\n",
    "# 0.1528 - baseline\n",
    "# 0.1693 - more vars\n",
    "# 0.1654 - new train test split to keep all player observations in one set and avoid overstating test performance\n",
    "# 0.2757 - training/evaluation done on rows where rolling metrics are populated\n",
    "# 0.2920 - teams labels and my team difficulty\n",
    "# 0.3835 - add team goals for/against\n",
    "# 0.3235 - average performance over several test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter(df, x, y, noise=False):\n",
    "    \"\"\"Scatter plot with random noise to facilitate comparing continuous predictions with integer scores\"\"\"\n",
    "    jitter = df[[x, y]]\n",
    "    \n",
    "    if noise:\n",
    "        nx = np.random.normal(0, 0.2, len(df)) \n",
    "        ny = np.random.normal(0, 0.2, len(df)) \n",
    "        jitter[x] += nx\n",
    "        jitter[y] += ny\n",
    "    \n",
    "    jitter.plot.scatter(x, y, xlim=(-2, 20), ylim=(-2, 20), figsize=(6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter tuning\n",
    "# results = []\n",
    "# for n in tqdm([2, 5, 10, 20, 50, 100, 200, 500, 1000]):\n",
    "#     reg = RandomForestRegressor(n_estimators=n, min_samples_split=100)\n",
    "#     reg.fit(X_train, y_train)\n",
    "#     results.append((n, reg.score(*train), reg.score(*test)))\n",
    "\n",
    "# pd.DataFrame(results, columns=['ntrees', 'train', 'test']).set_index('ntrees').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_points'] = reg.predict(df[covars])\n",
    "df['predicted_points'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ep_this = df.ep_this.astype(float)\n",
    "scatter(df, 'total_points', 'predicted_points', noise=True)\n",
    "scatter(df, 'total_points', 'ep_this', noise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_points(df, pid):\n",
    "    player = df[df.element==pid]\n",
    "    player.set_index('fixture').sort_index()[['predicted_points', 'total_points']].plot(\n",
    "        kind='bar', ylim=(-5, 25), title=f\"{player.web_name.iloc[0]} ({pid})\"\n",
    "    )\n",
    "\n",
    "for pid in df.element.unique()[:10]:\n",
    "    plot_predicted_points(df, pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid in [30, 80, 275, 262, 110, 245, 62, 272, 35, 144, 277, 359, 413, 337, 189]:\n",
    "    plot_predicted_points(df, pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict next fixture points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fpl21)",
   "language": "python",
   "name": "fpl21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
