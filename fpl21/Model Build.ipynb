{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "\n",
    "import json\n",
    "import random\n",
    "import matplotlib as pyplot\n",
    "from tqdm import tqdm\n",
    "\n",
    "from fpl21.utils import pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate base data set\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"players_list.json\", \"r\") as f:\n",
    "    players_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Players list file contains player attributes, fixtures list, historical fixtures and previous season performance\n",
    "players_list[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history is previous fictures with stats. fixtures is forward looking\n",
    "len(players_list[0]['history']), len(players_list[0]['fixtures'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Static player attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a player attrs dataframe\n",
    "keys = ['id', 'web_name', 'element_type', 'team', 'ep_this', 'ep_next']\n",
    "\n",
    "player_attrs = pd.DataFrame(\n",
    "    [[p[key] for key in keys] for p in players_list],\n",
    "    columns=keys)\n",
    "\n",
    "player_attrs.rename(columns={'id': 'element'}, inplace=True)\n",
    "#player_attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Historical match data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = [p['history'] for p in players_list]\n",
    "df = pd.DataFrame([x for sublist in hist for x in sublist])\n",
    "df.rename(columns={'was_home': 'is_home'}, inplace=True)\n",
    "df = df.merge(player_attrs, on='element')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixture data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_list[100]['fixtures'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work out team metrics per minute played. This handles transfers, varying number of fixtures and having a large\n",
    "# squad where lots of players don't get gametime and so have 0 points\n",
    "team_data = df.groupby('team').agg({\n",
    "    'total_points': np.sum,\n",
    "    'goals_scored': np.sum,\n",
    "    'goals_conceded': np.sum,\n",
    "    'minutes': np.sum\n",
    "})\n",
    "team_data['team_points_per_game'] = 90 * 11 * team_data.total_points / team_data.minutes\n",
    "team_data['team_goals_conceded_per_game'] = 90 * team_data.goals_conceded / team_data.minutes # already counted for every player\n",
    "team_data['team_goals_scored_per_game'] = 90 * 11 * team_data.goals_scored / team_data.minutes\n",
    "\n",
    "team_cols = ['team_points_per_game', 'team_goals_conceded_per_game', 'team_goals_scored_per_game']\n",
    "team_data = team_data[team_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(team_data, left_on='opponent_team', right_index=True) \\\n",
    "   .rename(columns={c: f\"opp_{c}\" for c in team_cols}) \\\n",
    "   .merge(team_data, left_on='team', right_index=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['element', 'web_name', 'fixture', 'round',]\n",
    "\n",
    "# Static player attributes\n",
    "player_cols = ['element_type']#, 'ep_this', 'ep_next']\n",
    "\n",
    "# Attributes from fixture data\n",
    "fixtures_cols = ['is_home', 'team', 'opponent_team'] + team_cols + [f\"opp_{c}\" for c in team_cols]\n",
    "\n",
    "# Historical match data - contains outcomes and things that are correlated, e.g. number of minutes played\n",
    "# Need to be lagged to use as predictors\n",
    "history_cols = ['total_points', 'bonus', 'bps', 'minutes', 'selected', 'transfers_in', 'transfers_out'] \n",
    "\n",
    "df = df[labels + player_cols + fixtures_cols + history_cols]\n",
    "\n",
    "# Others\n",
    "\n",
    "# 'goals_scored', 'assists', 'clean_sheets', 'goals_conceded',\n",
    "# 'own_goals', 'penalties_saved', 'penalties_missed', 'yellow_cards',\n",
    "# 'red_cards', 'saves', 'influence', 'creativity',\n",
    "# 'threat', 'ict_index', 'value', 'transfers_balance',\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.total_points.hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature generation\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_avg(df, window, col, default):\n",
    "    rolling = pd.Series(dtype='float64')\n",
    "    for x in df.element.unique():\n",
    "        rolling = rolling.append(df[df.element==x].sort_values('fixture').rolling(window)[col].mean().shift(1))\n",
    "    \n",
    "    # Fill nas with default val\n",
    "    rolling = rolling.fillna(default)\n",
    "    \n",
    "    rolling.name = f\"avg_{col}_L{window}\"\n",
    "    return df.merge(rolling, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in tqdm(history_cols): # we can use previous outcomes to predict next values\n",
    "    df = rolling_avg(df, 1, var, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_index().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use data where rolling metrics are populated\n",
    "df = df[df['round'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.element_type = df.element_type.astype(str)\n",
    "df.team = df.team.astype(str)\n",
    "df.opponent_team = df.opponent_team.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covars = [name for name in df.columns if name not in labels + history_cols]\n",
    "\n",
    "X = df[covars]\n",
    "y = df.total_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom train/test split to ensure all player observations in same set\n",
    "# To prevent overstating test performance\n",
    "elements = list(df.element.unique())\n",
    "test_elements = random.sample(elements, len(elements) // 4)\n",
    "mask = np.array([(x in test_elements) for x in df.element])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[~mask]\n",
    "y_train = y[~mask]\n",
    "X_test = X[mask]\n",
    "y_test = y[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = (X_train, y_train)\n",
    "test = (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = RandomForestRegressor(min_samples_split=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.fit(*train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(reg.feature_importances_, index=covars, columns=['importance']) \\\n",
    "    .sort_values('importance').plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train {reg.score(*train)}\")\n",
    "print(f\" Test {reg.score(*test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Log (Test performance)\n",
    "\n",
    "# 0.4000 - using ep_this, ep_next\n",
    "# 0.1528 - baseline\n",
    "# 0.1693 - more vars\n",
    "# 0.1654 - new train test split to keep all player observations in one set and avoid overstating test performance\n",
    "# 0.2757 - training/evaluation done on rows where rolling metrics are populated\n",
    "# 0.2920 - teams labels and my team difficulty\n",
    "# 0.3835 - add team goals for/against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter(df, x, y, noise=False):\n",
    "    \"\"\"Scatter plot with random noise to facilitate comparing continuous predictions with integer scores\"\"\"\n",
    "    jitter = df[[x, y]]\n",
    "    \n",
    "    if noise:\n",
    "        nx = np.random.normal(0, 0.2, len(df)) \n",
    "        ny = np.random.normal(0, 0.2, len(df)) \n",
    "        jitter[x] += nx\n",
    "        jitter[y] += ny\n",
    "\n",
    "    jitter.plot.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter tuning\n",
    "# results = []\n",
    "# for n in tqdm([2, 5, 10, 20, 50, 100, 200, 500, 1000]):\n",
    "#     reg = RandomForestRegressor(n_estimators=n, min_samples_split=100)\n",
    "#     reg.fit(X_train, y_train)\n",
    "#     results.append((n, reg.score(*train), reg.score(*test)))\n",
    "\n",
    "# pd.DataFrame(results, columns=['ntrees', 'train', 'test']).set_index('ntrees').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_points'] = reg.predict(df[covars])\n",
    "df['predicted_points'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(df, 'total_points', 'predicted_points', noise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_points(df, pid):\n",
    "    player = df[df.element==pid]\n",
    "    player.set_index('fixture').sort_index()[['predicted_points', 'total_points']].plot(\n",
    "        kind='bar', ylim=(-5, 25), title=f\"{player.web_name.iloc[0]} ({pid})\"\n",
    "    )\n",
    "\n",
    "for pid in df.element.unique()[:10]:\n",
    "    plot_predicted_points(df, pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid in [30, 80, 275, 262, 110, 245, 62, 272, 35, 144, 277, 359, 413, 337, 189]:\n",
    "    plot_predicted_points(df, pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fpl21)",
   "language": "python",
   "name": "fpl21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
